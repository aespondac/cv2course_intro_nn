{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!INFO]  \n",
    "> - Alfredo Daniel Esponda Cervantes\n",
    "> - 27/03/2025\n",
    "> - Red neuronal simple\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neuronal simple\n",
    "\n",
    "(Ejercicio, 4 puntos posibles)\n",
    "\n",
    "En este notebook programaremos una red neuronal simple utilizando numpy. El objetivo es que comprendamos la diferencia con el perceptrón. A diferencia del ejercicio anterior en este usaremos funciones de numpy.\n",
    "\n",
    "- *numpy.dot(a, b, out=None)* Dot product of two arrays. Parameters: a array_like First argument, b array_like Second argument.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/irvingvasquez/cv2course_intro_nn/blob/master/02_red_neuronal_simple.ipynb)\n",
    "\n",
    "@juan1rving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos paquetes\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular producto punto\n",
    "\n",
    "El primer paso es calcular el logit, *h*, a partir del producto punto. La fórmula explícita es la siguiente:\n",
    "\n",
    "$$ h = W X +b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (1 punto): Implementa la combinación lineal\n",
    "def combinacion_lineal (W , X , b):\n",
    "    h = np.dot(W, X) + b\n",
    "    return h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de activación\n",
    "\n",
    "Para este ejemplo utilizaremos la función sigmoide como función de activación.\n",
    "\n",
    "$$ f(x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-x} }  $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (1 punto): Implementar la función de activación.\n",
    "def sigmoide(h):\n",
    "    sg = 1 / (1 + np.exp(-h))\n",
    "    return sg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurona simple\n",
    "\n",
    "Implementación de la neurona simple mediante una función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def neurona(W, X, b, activacion):\n",
    "    h = np.dot(W, X) + b\n",
    "    return activacion(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probar la red\n",
    "\n",
    "Ahora definamos unos pesos y veamos el resultado de una pasada frontal (fordward pass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "0.4329070950345457\n"
     ]
    }
   ],
   "source": [
    "# Definamos unos pesos y sesgo\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "# TODO (1 punto): Realiza el pase frontal\n",
    "\n",
    "# define la función de activación a usar \n",
    "activation_func = sigmoide\n",
    "# calcula la salida\n",
    "output = neurona(weights, inputs, bias, activation_func)\n",
    "\n",
    "print('Output:')\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas extra\n",
    "- Define una función de activación tangente hiperbólica. \n",
    "- Contesta: ¿La tanh y sigmoide producen las mismas salidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "-0.26362483547220333\n"
     ]
    }
   ],
   "source": [
    "# TODO: Completar el código\n",
    "def tanh(h):\n",
    "    th = (np.exp(h) - np.exp(-h)) / (np.exp(h) + np.exp(-h))\n",
    "    return th\n",
    "\n",
    "# TODO (1 punto): Realiza de nuevo la inferencia con la nueva función de activación\n",
    "activation_func = tanh\n",
    "# calcula la salida\n",
    "output = neurona(weights, inputs, bias, activation_func)\n",
    "print('Output:')\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, la función sigmoide produce valores entre 0 y 1, la tangente hiperbólica produce valores entre -1 y 1. La salida de la tangente hiperbólica puede ser negativa y la salida de la sigmoide siempre será positiva. Además, la tangente hiperbólica tiene una pendiente más pronunciada en el origen, lo que puede hacerla más adecuada para diferentes tareas de aprendizaje profundo.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
