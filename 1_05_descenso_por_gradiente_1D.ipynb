{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al descenso por gradiente\n",
    "\n",
    "(Ejercicio, 1 punto posible)\n",
    "\n",
    "El descenso por gradiente se basa en la idea de que, para encontrar un mínimo local (o global) de una función de costo, es posible moverse iterativamente en la dirección opuesta al gradiente de la función en cada punto. El gradiente indica la dirección de mayor incremento de la función, por lo que moverse en la dirección contraria reduce el valor de la función.\n",
    "\n",
    "El descenso por gradiente sigue estos pasos:\n",
    "\n",
    "1. **Inicialización**:\n",
    "   Se comienza con una estimación inicial de los parámetros del modelo, que podrían ser asignados aleatoriamente. Estos parámetros son las variables que se ajustan para minimizar la función de costo.\n",
    "\n",
    "2. **Cálculo del Gradiente**:\n",
    "   En cada iteración, se calcula el gradiente de la función de costo con respecto a los parámetros. El gradiente es un vector que señala en la dirección de mayor incremento de la función.\n",
    "\n",
    "3. **Actualización de Parámetros**:\n",
    "   Los parámetros del modelo se ajustan moviéndose en la dirección opuesta al gradiente. Esto se hace de acuerdo con la siguiente fórmula:\n",
    "   $\n",
    "   \\theta := \\theta - \\eta \\nabla_{\\theta} f\n",
    "   $\n",
    "\n",
    "   Donde:\n",
    "   - $\\theta$ es el parámetro de la función.\n",
    "   - $\\nabla_{\\theta} f$ es el gradiente de la función con respecto a los parámetros.\n",
    "   - $\\eta $ es la **tasa de aprendizaje**, que controla el tamaño del paso que se da en cada iteración.\n",
    "\n",
    "4. **Repetición**:\n",
    "   Este proceso se repite hasta que se alcanza un criterio de parada, que puede ser:\n",
    "   - Que los cambios en los parámetros sean muy pequeños.\n",
    "   - Que el valor de la función de costo se estabilice.\n",
    "   - Un número máximo de iteraciones.\n",
    "\n",
    "jivg.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo usaremos la función:\n",
    "\n",
    "$$\n",
    "y = x^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#función objetivo:\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "# derivada de la función objetivo:\n",
    "def df(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda que la regla del descenso general es\n",
    "\n",
    "$$\n",
    "    \\theta = \\theta - tasa * derivada(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implemente la función del descenso por gradiente que calcula el siguiente valor del parámetro x usando la derivada de la función objetivo.\n",
    "def gradient_descent_update(x, learning_rate):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba del método de descenso por gradiente\n",
    "\n",
    "# Inicialización de x\n",
    "x = 5\n",
    "# Hiperparámetro de tasa de aprendizaje\n",
    "tasa = 0.1\n",
    "# Número de iteraciones\n",
    "n_iter = 10\n",
    "\n",
    "history = []\n",
    "history.append(x)\n",
    "for i in range(n_iter):\n",
    "    x = gradient_descent_update(x, tasa)\n",
    "    history.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x values\n",
    "x = np.linspace(-10, 10, 400)\n",
    "\n",
    "# Compute y values\n",
    "y = f(x)\n",
    "\n",
    "# Plot the function\n",
    "plt.plot(x, y, label='f(x) = x * x')\n",
    "plt.plot(history, [f(val) for val in history], 'ro', label='Puntos del descenso')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Descenso por gradiente para f(x) = x * x')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
